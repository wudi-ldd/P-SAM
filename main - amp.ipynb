{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import random\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate all hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bounding box\n",
    "def generate_mask_calculate_bounding_box(points, mask_image_resized):\n",
    "    x_coords = [p[0] for p in points]\n",
    "    y_coords = [p[1] for p in points]\n",
    "    min_x, max_x = min(x_coords), max(x_coords)\n",
    "    min_y, max_y = min(y_coords), max(y_coords)\n",
    "    # Create a new all-zero mask, the same size as mask_image_resized\n",
    "    mask = np.zeros_like(mask_image_resized, dtype=np.uint8)\n",
    "    mask[min_y:max_y, min_x:max_x] = np.where(mask_image_resized[min_y:max_y, min_x:max_x] > 0, 1, 0)\n",
    "    mask = np.where(mask == 0, -50, mask)\n",
    "    mask = np.where(mask == 1, 50, mask)\n",
    "    mask = cv2.resize(mask, (256, 256), interpolation=cv2.INTER_NEAREST)\n",
    "    return [min_x, min_y, max_x, max_y], mask\n",
    "\n",
    "def calculate_weighted_centroid(mask):\n",
    "    # Use connected components analysis to find labels for all masks\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=4)\n",
    "\n",
    "    # If there is only one mask, directly calculate its centroid\n",
    "    if num_labels == 2:\n",
    "        return calculate_single_mask_centroid(mask)\n",
    "\n",
    "    # Otherwise, find the largest mask\n",
    "    max_label = np.argmax(stats[1:, cv2.CC_STAT_AREA]) + 1\n",
    "    largest_mask = (labels == max_label).astype(np.uint8)\n",
    "\n",
    "    return calculate_single_mask_centroid(largest_mask)\n",
    "\n",
    "def calculate_single_mask_centroid(mask):\n",
    "    area = mask.sum()\n",
    "    # Calculate distance transform\n",
    "    dist_transform = cv2.distanceTransform(mask, cv2.DIST_L2, 5)\n",
    "    # Adjust weights using the square of the distance\n",
    "    adjusted_weights = np.square(dist_transform)\n",
    "\n",
    "    # Get the coordinates of the foreground pixels in the mask\n",
    "    y_indices, x_indices = np.where(mask > 0)\n",
    "\n",
    "    # Calculate weighted centroid\n",
    "    total_weight = np.sum(adjusted_weights[mask > 0])\n",
    "    if total_weight == 0 or np.isinf(total_weight) or np.isnan(total_weight):\n",
    "        return None\n",
    "    centroid_x = np.sum(x_indices * adjusted_weights[mask > 0]) / total_weight\n",
    "    centroid_y = np.sum(y_indices * adjusted_weights[mask > 0]) / total_weight\n",
    "    return [int(centroid_x), int(centroid_y)], area\n",
    "\n",
    "def calculate_foreground_centroid(bbox, polygon_points, original_height, original_width):\n",
    "    mask = np.zeros((original_height, original_width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, np.array([polygon_points], dtype=np.int32), 255)\n",
    "    x1, y1, x2, y2 = bbox  # Coordinates of the top left and bottom right corners\n",
    "    # Convert x1, y1, x2, y2 to integers\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    cropped_mask = mask[y1:y2, x1:x2]\n",
    "    # Calculate centroid\n",
    "    weighted_centroid = calculate_weighted_centroid(cropped_mask)\n",
    "    if weighted_centroid and cropped_mask[weighted_centroid[0][1], weighted_centroid[0][0]] > 0:\n",
    "        # If the weighted centroid is within the mask, return it directly\n",
    "        return weighted_centroid[0][0] + x1, weighted_centroid[0][1] + y1\n",
    "\n",
    "    # Divide the bounding box into four equal triangles and calculate their respective weighted centroids\n",
    "    triangles = [\n",
    "        np.array([[0, 0], [w, 0], [0, h]], dtype=np.int32),\n",
    "        np.array([[w, 0], [w, h], [0, h]], dtype=np.int32),\n",
    "        np.array([[0, 0], [w, 0], [w, h]], dtype=np.int32),\n",
    "        np.array([[0, 0], [0, h], [w, h]], dtype=np.int32)\n",
    "    ]\n",
    "    centroids = []\n",
    "    for tri in triangles:\n",
    "        # Create triangle mask\n",
    "        tri_mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillConvexPoly(tri_mask, tri, 1)\n",
    "        # Calculate the weighted centroid within the triangle area\n",
    "        tri_weighted_centroid = calculate_weighted_centroid(cropped_mask * tri_mask)\n",
    "        if tri_weighted_centroid and (cropped_mask * tri_mask)[tri_weighted_centroid[0][1], tri_weighted_centroid[0][0]] > 0:\n",
    "            # Calculate the coordinates relative to the original image\n",
    "            tri_weighted_centroid = ([tri_weighted_centroid[0][0] + x1, tri_weighted_centroid[0][1] + y1], tri_weighted_centroid[1])\n",
    "            centroids.append((tri_weighted_centroid[0], tri_weighted_centroid[1]))\n",
    "    if centroids:\n",
    "        # Choose the centroid of the region with the largest area\n",
    "        max_area_centroid = max(centroids, key=lambda x: x[1])[0]\n",
    "        return max_area_centroid\n",
    "    \n",
    "    # If no suitable centroid is found, return None\n",
    "    return None\n",
    "\n",
    "def calculate_background_centroid(bbox, polygon_points, original_height, original_width):\n",
    "    mask = np.zeros((original_height, original_width), dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, np.array([polygon_points], dtype=np.int32), 1)\n",
    "    x1, y1, x2, y2 = bbox  # Coordinates of the top left and bottom right corners\n",
    "    # Convert x1, y1, x2, y2 to integers\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    cropped_mask = mask[y1:y2, x1:x2]\n",
    "    inverted_mask = 1 - cropped_mask\n",
    "\n",
    "    # Adjust to within the bounding box\n",
    "    adjusted_corners = [\n",
    "        (min(x1 + 1, original_width - 1), min(y1 + 1, original_height - 1)),  # Top left corner\n",
    "        (max(x2 - 1, 0), min(y1 + 1, original_height - 1)),                   # Top right corner\n",
    "        (min(x1 + 1, original_width - 1), max(y2 - 1, 0)),                    # Bottom left corner\n",
    "        (max(x2 - 1, 0), max(y2 - 1, 0))                                      # Bottom right corner\n",
    "    ]\n",
    "\n",
    "    # Convert the corner points' coordinates relative to the original image to relative to the bounding box\n",
    "    adjusted_corners = [(cx - x1, cy - y1) for cx, cy in adjusted_corners]\n",
    "\n",
    "    # Use connected components analysis to find labels for all backgrounds\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(inverted_mask, connectivity=4)\n",
    "    # Store centroids and areas of background regions that contain corners\n",
    "    corner_centroids = []\n",
    "    for corner in adjusted_corners:\n",
    "        cx, cy = corner\n",
    "        label_at_corner = labels[cy, cx]\n",
    "        if label_at_corner != 0:  # Ensure the corner is in a background region\n",
    "            corner_mask = (labels == label_at_corner).astype(np.uint8) \n",
    "            result = calculate_single_mask_centroid(corner_mask)\n",
    "            if result is not None:\n",
    "                centroid, area = result\n",
    "                if centroid and inverted_mask[centroid[1], centroid[0]] > 0:\n",
    "                    corner_centroids.append((centroid, area))\n",
    "\n",
    "    # Choose the centroid of the region with the largest area\n",
    "    if corner_centroids:\n",
    "        max_area_centroid = max(corner_centroids, key=lambda x: x[1])[0]\n",
    "        # Calculate the coordinates relative to the original image\n",
    "        max_area_centroid = (max_area_centroid[0] + x1, max_area_centroid[1] + y1)\n",
    "        return max_area_centroid\n",
    "    else:\n",
    "        print(\"Unable to find a background centroid, randomly selecting a point\")\n",
    "        # Choose the largest area connected region\n",
    "        max_area_label = np.argmax(stats[1:, 4]) + 1  # Ignore background label\n",
    "        max_area_mask = (labels == max_area_label).astype(np.uint8)\n",
    "\n",
    "        # Randomly select a point from the largest area\n",
    "        possible_points = np.argwhere(max_area_mask)\n",
    "        if len(possible_points) > 0:\n",
    "            random_point = random.choice(possible_points)\n",
    "            random_point_global = (random_point[1] + x1, random_point[0] + y1)\n",
    "            return random_point_global\n",
    "\n",
    "# Adjust coordinates\n",
    "def adjust_coordinates(coordinates, original_width, original_height, new_width, new_height):\n",
    "    scale_x = new_width / original_width\n",
    "    scale_y = new_height / original_height\n",
    "    return [int(coord * scale_x if i % 2 == 0 else coord * scale_y) for i, coord in enumerate(coordinates)]\n",
    "\n",
    "def enlarge_bbox_and_adjust_coordinates(bbox, original_width, original_height, new_width, new_height, scale_factor=1.2):\n",
    "    # Calculate the center point of the bounding box\n",
    "    bbox_center_x = (bbox[0] + bbox[2]) / 2\n",
    "    bbox_center_y = (bbox[1] + bbox[3]) / 2\n",
    "\n",
    "    # Enlarge bounding box coordinates\n",
    "    enlarged_bbox = []\n",
    "    for i, coord in enumerate(bbox):\n",
    "        if i % 2 == 0:  # X coordinate\n",
    "            new_coord = (coord - bbox_center_x) * scale_factor + bbox_center_x\n",
    "        else:           # Y coordinate\n",
    "            new_coord = (coord - bbox_center_y) * scale_factor + bbox_center_y\n",
    "        enlarged_bbox.append(int(new_coord))\n",
    "\n",
    "    # Scale coordinates proportionally\n",
    "    adjusted_bbox = adjust_coordinates(enlarged_bbox, original_width, original_height, new_width, new_height)\n",
    "\n",
    "    # Ensure the bounding box does not exceed the new image boundaries\n",
    "    adjusted_bbox[0] = max(0, min(adjusted_bbox[0], new_width - 1))  # x1\n",
    "    adjusted_bbox[1] = max(0, min(adjusted_bbox[1], new_height - 1)) # y1\n",
    "    adjusted_bbox[2] = max(0, min(adjusted_bbox[2], new_width - 1))  # x2\n",
    "    adjusted_bbox[3] = max(0, min(adjusted_bbox[3], new_height - 1)) # y2\n",
    "\n",
    "    return adjusted_bbox\n",
    "\n",
    "def process_json_file(json_path, mask_image):\n",
    "    with open(json_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    original_width, original_height = data['imageWidth'], data['imageHeight']\n",
    "    new_width, new_height = 1024, 1024\n",
    "    mask_image_resized = cv2.resize(mask_image, (original_width, original_height), interpolation=cv2.INTER_NEAREST)\n",
    "    bounding_boxes = []\n",
    "    sampled_foreground_points = []\n",
    "    sampled_background_points = []\n",
    "    masks = []\n",
    "\n",
    "    for shape in data['shapes']:\n",
    "        bbox, mask = generate_mask_calculate_bounding_box(shape['points'], mask_image_resized)\n",
    "        foreground_point = calculate_foreground_centroid(bbox, shape['points'], original_height, original_width)\n",
    "        background_point = calculate_background_centroid(bbox, shape['points'], original_height, original_width)\n",
    "        adjusted_bbox = enlarge_bbox_and_adjust_coordinates(bbox, original_width, original_height, new_width, new_height)\n",
    "        adjusted_foreground_point = adjust_coordinates(foreground_point, original_width, original_height, new_width, new_height)\n",
    "        adjusted_background_point = adjust_coordinates(background_point, original_width, original_height, new_width, new_height)\n",
    "        masks.append(mask)\n",
    "        bounding_boxes.append(adjusted_bbox)\n",
    "        sampled_foreground_points.append(adjusted_foreground_point)\n",
    "        sampled_background_points.append(adjusted_background_point)\n",
    "\n",
    "    return masks, bounding_boxes, sampled_foreground_points, sampled_background_points\n",
    "\n",
    "# Path to the folder containing JSON files\n",
    "folder_path = 'json'\n",
    "prompt_masks_path = 'prompt_mask'\n",
    "file_list = os.listdir(folder_path)\n",
    "random.shuffle(file_list)\n",
    "image_data = {}\n",
    "# Get all filenames in the folder\n",
    "for filename in file_list:\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        mask_file_path = os.path.join(prompt_masks_path, filename.split('.')[0] + '.jpg')\n",
    "        # Read the mask image\n",
    "        mask_image = cv2.imread(mask_file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask_image is None:\n",
    "            raise FileNotFoundError(\"Mask image file not found\")\n",
    "        masks, bounding_boxes, foreground_points, background_points = process_json_file(file_path, mask_image)\n",
    "        image_name = filename.split('.')[0]\n",
    "        image_data[image_name] = {\n",
    "            'masks': masks,\n",
    "            'bounding_boxes': bounding_boxes,\n",
    "            'foreground_points': foreground_points,\n",
    "            'background_points': background_points,\n",
    "        }\n",
    "\n",
    "# Output results\n",
    "for image_name, data in image_data.items():\n",
    "    print(f\"Image: {image_name}\")\n",
    "    print(\"Number of masks:\", len(data['masks']))\n",
    "    print(\"Number of bounding boxes:\", len(data['bounding_boxes']))\n",
    "    print(\"Number of foreground points:\", len(data['foreground_points']))\n",
    "    print(\"Number of background points:\", len(data['background_points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_path = 'mask'\n",
    "images_path = 'datasets/JPEGImages'\n",
    "for key in image_data.keys():\n",
    "    image_data[key]['ground_truth_mask'] = []\n",
    "    image_data[key]['prompt_mask'] = []\n",
    "\n",
    "for filename in os.listdir(masks_path):\n",
    "    if filename.endswith('.png'):\n",
    "        file_path = os.path.join(masks_path, filename)\n",
    "        # Read as a single-channel binary image\n",
    "        mask = cv2.imread(file_path)\n",
    "        gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = cv2.resize(gray_mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "        # mask[mask > 0] = 1\n",
    "        # Visualize mask\n",
    "        # plt.imshow(mask)\n",
    "        # plt.show()\n",
    "        image_name = filename.split('.')[0]\n",
    "        image_data[image_name]['ground_truth_mask'] = mask\n",
    "\n",
    "for filename in os.listdir(prompt_masks_path):\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        file_path = os.path.join(prompt_masks_path, filename)\n",
    "        # Read as a single-channel binary image\n",
    "        mask = cv2.imread(file_path)\n",
    "        gray_mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = cv2.resize(gray_mask, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "        mask[mask > 0] = 1\n",
    "        # Visualize mask\n",
    "        # plt.imshow(mask, cmap='gray')\n",
    "        # plt.show()\n",
    "        image_name = filename.split('.')[0]\n",
    "        image_data[image_name]['prompt_mask'] = mask\n",
    "\n",
    "for filename in os.listdir(images_path):\n",
    "    if filename.endswith('.jpg'):\n",
    "        file_path = os.path.join(images_path, filename)\n",
    "        # Read the image\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (1024, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "        # Visualize image\n",
    "        # plt.imshow(image)\n",
    "        # plt.show()\n",
    "        image_name = filename.split('.')[0]\n",
    "        image_data[image_name]['image'] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'sam_vit_b_01ec64.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "sam_model.to(device)\n",
    "sam_model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the images\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "\n",
    "transformed_data = defaultdict(dict)\n",
    "for filename in image_data.keys():\n",
    "  image = cv2.imread(f'datasets/JPEGImages/{filename}.jpg')\n",
    "  image=cv2.resize(image,(1024,1024),interpolation=cv2.INTER_NEAREST)\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "  transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "  input_image = transform.apply_image(image)\n",
    "  input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "  transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "  \n",
    "  input_image = sam_model.preprocess(transformed_image)\n",
    "  original_image_size = image.shape[:2]\n",
    "  input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "  transformed_data[filename]['image'] = input_image\n",
    "  transformed_data[filename]['input_size'] = input_size\n",
    "  transformed_data[filename]['original_image_size'] = original_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "# Get the parameters for mask_decoder and prompt_encoder\n",
    "mask_decoder_params = sam_model.mask_decoder.parameters()\n",
    "prompt_encoder_params = sam_model.prompt_encoder.parameters()\n",
    "image_encoder_params = sam_model.image_encoder.parameters()\n",
    "# Combine parameters\n",
    "# all_params = list(mask_decoder_params) + list(prompt_encoder_params) + list(image_encoder_params)\n",
    "all_params = list(prompt_encoder_params)\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(all_params, lr=lr, weight_decay=wd)\n",
    "# Assuming optimizer is the optimizer you have already defined\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-6)\n",
    "\n",
    "class CombinedDiceCrossEntropyFocalLoss(nn.Module):\n",
    "    def __init__(self, dice_loss_weight=1/3, focal_loss_weight=1/3, mse_loss_weight=1/3, alpha=0.8, gamma=2.0):\n",
    "        super(CombinedDiceCrossEntropyFocalLoss, self).__init__()\n",
    "        self.dice_loss_weight = dice_loss_weight\n",
    "        self.focal_loss_weight = focal_loss_weight\n",
    "        self.mse_loss_weight = mse_loss_weight\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, y_true, smooth=1.0):\n",
    "        # Dice Loss calculation\n",
    "        probs = torch.sigmoid(logits)\n",
    "        y_true_f = y_true.view(-1)\n",
    "        probs_f = probs.view(-1)\n",
    "        intersection = torch.sum(y_true_f * probs_f)\n",
    "        dice_loss = 1 - (2. * intersection + smooth) / (torch.sum(y_true_f) + torch.sum(probs_f) + smooth)\n",
    "        \n",
    "        # Focal Loss calculation\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(logits.view(-1), y_true_f, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss) # Prevents nans when probability 0\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        # MSE Loss calculation\n",
    "        mse_loss = F.mse_loss(probs_f, y_true_f)\n",
    "        \n",
    "        combined_loss = (self.dice_loss_weight * dice_loss +\n",
    "                         self.focal_loss_weight * torch.mean(focal_loss) + self.mse_loss_weight * mse_loss)\n",
    "        return combined_loss\n",
    "\n",
    "loss_fn = CombinedDiceCrossEntropyFocalLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Initialize GradScaler\n",
    "scaler = GradScaler()\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "with open('train.txt', 'r') as file:\n",
    "    train_filenames = file.read().splitlines()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  epoch_losses = []\n",
    "  for filename in tqdm(train_filenames, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True):\n",
    "    # Skip if the number of bounding boxes is greater than 150 or if it's empty\n",
    "    if len(image_data[filename]['bounding_boxes']) > 150 or len(image_data[filename]['bounding_boxes']) == 0:\n",
    "        continue\n",
    "    input_image = transformed_data[filename]['image'].to(device)\n",
    "    input_size = transformed_data[filename]['input_size']\n",
    "    original_image_size = transformed_data[filename]['original_image_size']\n",
    "    # Use autocast context manager for forward pass\n",
    "    with autocast():\n",
    "        image_embedding = sam_model.image_encoder(input_image)\n",
    "        # Create two empty lists for storing sparse_embeddings and dense_embeddings\n",
    "        sparse_embeddings_list = []\n",
    "        dense_embeddings_list = []\n",
    "        for i in range(len(image_data[filename]['bounding_boxes'])):\n",
    "          prompt_box = np.array(image_data[filename]['bounding_boxes'][i])\n",
    "          box = transform.apply_boxes(prompt_box, original_image_size)\n",
    "          box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "          box_torch = box_torch[None, :]\n",
    "          mask_torch = torch.from_numpy(image_data[filename]['masks'][i]).type(torch.float32).to(device)\n",
    "          mask_torch = mask_torch.unsqueeze(0).unsqueeze(0)  # Add a dimension at 0, becomes [1,1, height, width]\n",
    "          point_torch = torch.tensor([[image_data[filename]['foreground_points'][i],image_data[filename]['background_points'][i]]],dtype=torch.float32).to(device)\n",
    "          type_torch = torch.tensor([[1,0]],dtype=torch.float32).to(device)\n",
    "          points_torch = [point_torch,type_torch]\n",
    "          sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "            points=points_torch,\n",
    "            boxes=box_torch,\n",
    "            masks=mask_torch,\n",
    "            )\n",
    "          # Add the obtained sparse_embeddings and dense_embeddings to their respective lists\n",
    "          sparse_embeddings_list.append(sparse_embeddings)\n",
    "          dense_embeddings_list.append(dense_embeddings)\n",
    "        # Concatenate all elements in sparse_embeddings_list and dense_embeddings_list\n",
    "        sparse_embeddings_all = torch.cat(sparse_embeddings_list, dim=0)\n",
    "        dense_embeddings_all = torch.cat(dense_embeddings_list, dim=0)\n",
    "        low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "          image_embeddings=image_embedding,\n",
    "          image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "          sparse_prompt_embeddings=sparse_embeddings_all,\n",
    "          dense_prompt_embeddings=dense_embeddings_all,\n",
    "          multimask_output=True,\n",
    "        )\n",
    "        upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "        # Calculate the index of the highest score for each channel\n",
    "        max_score_indices = torch.argmax(iou_predictions, dim=1)\n",
    "        # Initialize an empty tensor for storing selected masks\n",
    "        selected_masks = torch.zeros_like(upscaled_masks[:, 0, :, :]).to(device)  # shape [C, W, H]\n",
    "        # Select the highest scoring mask channel\n",
    "        for i, index in enumerate(max_score_indices):\n",
    "            selected_masks[i] = upscaled_masks[i, index]\n",
    "        # Merge masks to get a binary mask\n",
    "        binary_mask, _ = torch.max(selected_masks, dim=0, keepdim=True)\n",
    "        gt_mask_resized = torch.from_numpy(image_data[filename]['ground_truth_mask']).unsqueeze(0).to(device)\n",
    "        gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "        prompt_mask = torch.from_numpy(image_data[filename]['prompt_mask']).unsqueeze(0).to(device)\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "\n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Scale the loss and perform backpropagation using GradScaler\n",
    "    scaler.scale(loss).backward()\n",
    "    # Unscale the gradients and perform optimizer step using GradScaler\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    epoch_losses.append(loss.item())\n",
    "  losses.append(epoch_losses)\n",
    "  current_lr = optimizer.param_groups[0]['lr']\n",
    "  print(f'EPOCH: {epoch}, Mean loss: {mean(epoch_losses)}, Current LR: {current_lr}')\n",
    "  # Check and update best loss\n",
    "  if mean(epoch_losses) < best_loss:\n",
    "      best_loss = mean(epoch_losses)\n",
    "      # Save model weights with lowest loss\n",
    "      torch.save(sam_model.state_dict(), 'sam_vit_b_best_loss.pth')\n",
    "      print(\"Saved model with lower loss:\", best_loss)\n",
    "  # Update learning rate at the end of each epoch\n",
    "  scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_losses = [mean(x) for x in losses]\n",
    "mean_losses\n",
    "\n",
    "plt.plot(list(range(len(mean_losses))), mean_losses)\n",
    "plt.title('Mean epoch loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val.txt', 'r') as file:\n",
    "    val_filenames = file.read().splitlines()\n",
    "predict_checkpoint = 'sam_vit_b_best_loss.pth'\n",
    "# Load the model for prediction with the specified checkpoint\n",
    "sam_model_predict = sam_model_registry[model_type](checkpoint=predict_checkpoint)\n",
    "sam_model_predict.to(device)\n",
    "# Initialize the predictor\n",
    "predictor = SamPredictor(sam_model_predict)\n",
    "for filename in val_filenames:\n",
    "    # Skip if bounding boxes are empty\n",
    "    if len(image_data[filename]['bounding_boxes']) == 0:\n",
    "        continue\n",
    "    predicted_masks = []\n",
    "    input_image = image_data[filename]['image']\n",
    "    original_image_size = transformed_data[filename]['original_image_size']\n",
    "    predictor.set_image(input_image)\n",
    "    for i in range(len(image_data[filename]['bounding_boxes'])):\n",
    "        prompt_box = image_data[filename]['bounding_boxes'][i]\n",
    "        mask = image_data[filename]['masks'][i]\n",
    "        foreground_point = image_data[filename]['foreground_points'][i]\n",
    "        background_point = image_data[filename]['background_points'][i]\n",
    "        prompt_box = np.array(prompt_box)\n",
    "        prompt_point = [foreground_point, background_point]\n",
    "        prompt_point = np.array(prompt_point, dtype=float)\n",
    "        input_label = np.array([1, 0])\n",
    "        masks = np.array(mask, dtype=float)\n",
    "        masks, score, logits = predictor.predict(\n",
    "            point_coords=prompt_point,\n",
    "            point_labels=input_label,\n",
    "            box=prompt_box,\n",
    "            mask_input=masks[None, :, :],\n",
    "            multimask_output=True,\n",
    "        )\n",
    "        # Find the mask with the highest score\n",
    "        max_score = np.argmax(score)\n",
    "        predicted_masks.append(masks[max_score])\n",
    "    # Merge into a single mask, values greater than 1 after merging are set to 1\n",
    "    predicted_mask = np.array(predicted_masks).sum(axis=0)\n",
    "    predicted_mask = np.where(predicted_mask > 1, 1, predicted_mask)\n",
    "    # Save as a binary mask\n",
    "    predicted_mask = cv2.resize(predicted_mask, (1360, 1024), interpolation=cv2.INTER_NEAREST)\n",
    "    cv2.imwrite(f'predict/{filename}.png', predicted_mask * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define a function to calculate metrics for a single image\n",
    "def calculate_metrics(gt_image, pred_image):\n",
    "    ## Resize images to match the smaller one's dimensions\n",
    "    if gt_image.size != pred_image.size:\n",
    "        new_size = (min(gt_image.size[0], pred_image.size[0]), min(gt_image.size[1], pred_image.size[1]))\n",
    "        gt_image = gt_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "        pred_image = pred_image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Convert images to grayscale if they are not\n",
    "    if gt_image.mode != 'L':\n",
    "        gt_image = gt_image.convert('L')\n",
    "    if pred_image.mode != 'L':\n",
    "        pred_image = pred_image.convert('L')\n",
    "\n",
    "    # Load images and convert them to NumPy arrays\n",
    "    gt = np.array(gt_image)\n",
    "    pred = np.array(pred_image)\n",
    "\n",
    "    # Ensure the dimensions of the images are the same\n",
    "    if gt.shape != pred.shape:\n",
    "        raise ValueError(\"Image dimensions do not match\")\n",
    "\n",
    "    # Calculate True Positives\n",
    "    true_positives = np.sum(np.logical_and(gt == 255, pred == 255))\n",
    "\n",
    "    # Calculate False Positives\n",
    "    false_positives = np.sum(np.logical_and(gt == 0, pred == 255))\n",
    "\n",
    "    # Calculate False Negatives\n",
    "    false_negatives = np.sum(np.logical_and(gt == 255, pred == 0))\n",
    "\n",
    "    # Calculate True Negatives\n",
    "    true_negatives = np.sum(np.logical_and(gt == 0, pred == 0))\n",
    "\n",
    "    # Calculate IoU (Intersection over Union)\n",
    "    iou = true_positives / (true_positives + false_positives + false_negatives)\n",
    "\n",
    "    # Calculate Dice coefficient\n",
    "    dice = 2 * true_positives / (2 * true_positives + false_positives + false_negatives)\n",
    "\n",
    "    # Calculate Precision\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    else:\n",
    "        precision =1\n",
    "\n",
    "    # Calculate FPR (False Positive Rate)\n",
    "    fpr = false_positives / (false_positives + true_negatives)\n",
    "\n",
    "    # Calculate PA\n",
    "    pa = (true_positives + true_negatives) / (true_positives + false_positives + false_negatives + true_negatives)\n",
    "\n",
    "    return iou, dice, precision, fpr,pa\n",
    "\n",
    "# Define folder paths\n",
    "gt_folder = \"mask\"\n",
    "pred_folder = \"predict\"\n",
    "\n",
    "# Get all image filenames in the folder\n",
    "gt_files = os.listdir(gt_folder)\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "ious = []\n",
    "dices = []\n",
    "precisions = []\n",
    "fprs = []\n",
    "pas=[]\n",
    "\n",
    "# Iterate over each image file and calculate metrics\n",
    "for filename in gt_files:\n",
    "    gt_path = os.path.join(gt_folder, filename)\n",
    "    pred_path = os.path.join(pred_folder, filename)\n",
    "\n",
    "    # Open image files\n",
    "    # Skip if unable to open\n",
    "    try:\n",
    "        gt_image = Image.open(gt_path)\n",
    "        pred_image = Image.open(pred_path)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    iou, dice, precision, fpr, pa = calculate_metrics(gt_image, pred_image)\n",
    "\n",
    "    ious.append(iou)\n",
    "    dices.append(dice)\n",
    "    precisions.append(precision)\n",
    "    fprs.append(fpr)\n",
    "    pas.append(pa)\n",
    "\n",
    "# Calculate averages\n",
    "mean_iou = np.mean(ious)\n",
    "mean_dice = np.mean(dices)\n",
    "mean_precision = np.mean(precisions)\n",
    "mean_fpr = np.mean(fprs)\n",
    "mean_pa=np.mean(pa)\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(f\"IoU: {mean_iou:.8f}\")\n",
    "print(f\"Dice Coefficient: {mean_dice:.8f}\")\n",
    "print(f\"Precision: {mean_precision:.8f}\")\n",
    "print(f\"False Positive Rate: {mean_fpr:.8f}\")\n",
    "print(f\"PA: {mean_pa:.8f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "samlabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
